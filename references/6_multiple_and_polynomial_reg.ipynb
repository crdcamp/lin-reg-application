{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2626358b",
   "metadata": {},
   "source": [
    "# Multiple and Polynomial Regression\n",
    "\n",
    "[Resource](https://harvard-iacs.github.io/2018-CS109A/labs/lab-4/solutions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d47dc",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "* Implement arbitrary multiple regression models in both sklearn and statsmodels.\n",
    "* Interpret the coefficient estimates produced by each model, including transformed and dummy variables.\n",
    "\n",
    "`statsmodels` is focused on the **inference task**: guess good values for the betas and discuss how certain you are in those answers.\n",
    "\n",
    "`sklearn` is focused on the **prediction task**: Given new data, guess what the response value is. As a result, `statsmodels` has lots of tools to discuss confidence, but isn't great at dealing with test sets. `sklearn` is great at test sets and validations, but can't really discuss uncertainty in the parameters or predictions. In short:\n",
    "* `sklearn` is about putting a line through it and predicting new values using that line. If the line gives good predictions on the test set, who  cares about anything else?\n",
    "* `statsmodels` assumes more about how the data were generated, and (if the assumptions are correct) can tell you about uncertainty in the results.\n",
    "\n",
    "## Some terms\n",
    "* **R-squared**: An interpretable summary of how well the model did. 1 is perfect, 0 is a trivial baseline model, negative is worse than the trivial model.\n",
    "* **F-statistic**: A value testing whether we're likely to see these results (or even stronger ones) if none of the predictors actually mattered.\n",
    "* **Prob (F-statistic)**: The probability that we'd see these results (or even stronger ones) if none of the predictors actually mattered. If this probability is small then either A) some combination of predictors actually matters of B) something rather unlikely has happened.\n",
    "* **coef**: The estimate of each beta. This has several components:\n",
    "    * **std err**: The amount we'd expect this value to wiggle if we re-did the data collection and re-ran our model. More data tends to make this wiggle smaller, but sometimes the collected data just isn't enough to pin down a particular value.\n",
    "    * **t and P>|t|**: Similar to the F-statistic, these measure the probability of seeing coefficients this big (or even bigger) if the given variable didn't actually matter. Small probability doesn't necessarily mean the value matters.\n",
    "    * **[0.025 0.975]**: Endpoints of the 95% confidence interval. This is an interval drawn in a clever way and which gives us an idea of where the true beta value might plausibly live."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin-reg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
