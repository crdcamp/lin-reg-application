{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818d9149",
   "metadata": {},
   "source": [
    "# Model Selection and Information Criteria\n",
    "\n",
    "Refer to [the slides and other info](https://harvard-iacs.github.io/2018-CS109A/a-sections/a-section-2/). This is a conceptual lesson and does not involve coding. However, I think it'd be wise to apply these concepts from the slides and paper here to be ready for the next section.\n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b5930",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "First thing's first, **likelihood and probability are interrelated, but not synonymous** in the realm of statistics.\n",
    "\n",
    "**Likelihood** refers to how well a sample provides support for particular values of a parameter in a model.\n",
    "\n",
    "**Probability** refers to the chance that a particular outcome occurs based on the values of parameters in a model.\n",
    "\n",
    "## Statement of the Problem\n",
    "\n",
    "[Resource](https://online.stat.psu.edu/stat415/lesson/1/1.2)\n",
    "\n",
    "Suppose we have a random sample <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "</math> whose assumed probability distribution depends on some unknown parameter <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x3B8;</mi>\n",
    "</math>. Our primary goal here will be to find a point estimator <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>u</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>, such that <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>u</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math> is a \"good\" point estimate of <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x3B8;</mi>\n",
    "</math>, where <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "</math> are the observed values of the random sample.\n",
    "\n",
    "**A note from [this video](https://www.youtube.com/watch?v=k5sbE1_MDwU):** When a random variable is a capital letter (X), we are conveying that this object can take on different values (we haven't observed it yet. It's still random). When we see a lower case (x), we've actually observed a value from the random variable, and is therefore no longer random.\n",
    "\n",
    "All random variables (X) have a function with\n",
    "1. An input: the random variable and\n",
    "1. An output: how **likely** we are to observe the input number\n",
    "\n",
    "All random variables have such a function, that takes the form of either the probability density function (used for continuous random variables) or the probability mass function (discrete random variables). These are both known as **probability distributions**, which tells us about the structure in the randomness.\n",
    "\n",
    "Probabilities for a probability density function (the function we're focused on for this unit's purposes) can come from **point values**, such as f(X = x) or **intervals of values**, f(x1 <= X <= x2).\n",
    "\n",
    "The main idea here is that even though it's impossible to predict values of a random variable ahead of time, **the probability distribution tell us that, over many observations, the *frequency* that they appear will be predictable**. This is what we mean by *structure* - the predictability of the frequency that a random variable will appear, despite randomness!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
