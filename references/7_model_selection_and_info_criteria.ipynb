{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "818d9149",
   "metadata": {},
   "source": [
    "# Model Selection and Information Criteria\n",
    "\n",
    "Refer to [the slides and other info](https://harvard-iacs.github.io/2018-CS109A/a-sections/a-section-2/). This is a conceptual lesson and does not involve coding. However, I think it'd be wise to apply these concepts from the slides and paper here to be ready for the next section.\n",
    "\n",
    "Let's begin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b5930",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "First thing's first, **likelihood and probability are interrelated, but not synonymous** in the realm of statistics.\n",
    "\n",
    "**Likelihood** refers to how well a sample provides support for particular values of a parameter in a model.\n",
    "\n",
    "**Probability** refers to the chance that a particular outcome occurs based on the values of parameters in a model.\n",
    "\n",
    "**The critical distinction:**\n",
    "\n",
    "* Probability: P(X=x|θ) treated as a function of x (θ is fixed)\n",
    "* Likelihood: P(X=x|θ) treated as a function of θ (x is fixed/observed)\n",
    "\n",
    "## Statement of the Problem\n",
    "\n",
    "[PSU Resource](https://online.stat.psu.edu/stat415/lesson/1/1.2)\n",
    "\n",
    "Suppose we have a random sample <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "</math> whose assumed probability distribution depends on some unknown parameter <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x3B8;</mi>\n",
    "</math>. Our primary goal here will be to find a point estimator <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>u</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>, such that <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>u</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math> is a \"good\" point estimate of <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x3B8;</mi>\n",
    "</math>, where <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "</math> are the observed values of the random sample.\n",
    "\n",
    "**Some notes from [this video](https://www.youtube.com/watch?v=k5sbE1_MDwU):** When a random variable is a capital letter (X), we are conveying that this object can take on different values (we haven't observed it yet. It's still random). When we see a lower case (x), we've actually observed a value from the random variable, and it is therefore no longer random.\n",
    "\n",
    "All random variables (X) have a function with\n",
    "1. An input: the random variable and\n",
    "1. An output: how **likely** we are to observe the input number\n",
    "\n",
    "All random variables have such a function that takes the form of either the probability density function (used for continuous random variables) or the probability mass function (discrete random variables). These are both known as **probability distributions**, which tells us about the structure in the randomness.\n",
    "\n",
    "Probabilities for a probability density function (the function we're focused on for this unit's purposes) can come from **point values**, such as f(X = x) or **intervals of values**, f(x1 <= X <= x2).\n",
    "\n",
    "The main idea here is that even though it's impossible to predict values of a random variable ahead of time, **the probability distribution tell us that, over many observations, the *frequency* that they appear will be predictable**. This is what we mean by *structure* - the predictability of the frequency that a random variable will appear despite randomness!\n",
    "\n",
    "To wrap these ideas up: **probability distributions of statistics can tell us what values we are likely or unlikely to observe**.\n",
    "\n",
    "Now back to the [PSU resource](https://online.stat.psu.edu/stat415/lesson/1/1.2):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0c7871",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The Basic Idea\n",
    "\n",
    "It seems reasonable that a good estimate of the unknown parameter <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x3B8;</mi>\n",
    "</math> would be the value of <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>&#x3B8;</mi>\n",
    "</math> that maximizes the **likelihood** of getting the data we observed. Suppose we have a random sample <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "</math> for which the PDF of each <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>i</mi>\n",
    "  </msub>\n",
    "</math> is <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>i</mi>\n",
    "  </msub>\n",
    "  <mo>;</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>. Then, the joint probability mass (or density) function of <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "</math>, which is called <math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>L</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math> is:\n",
    "\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>L</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>P</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>,</mo>\n",
    "  <mo>&#x2026;</mo>\n",
    "  <mo>,</mo>\n",
    "  <msub>\n",
    "    <mi>X</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>1</mn>\n",
    "  </msub>\n",
    "  <mo>;</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>&#x22C5;</mo>\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mn>2</mn>\n",
    "  </msub>\n",
    "  <mo>;</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>&#x22EF;</mo>\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>n</mi>\n",
    "  </msub>\n",
    "  <mo>;</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <munderover>\n",
    "    <mo data-mjx-texclass=\"OP\" movablelimits=\"false\">&#x220F;</mo>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mi>i</mi>\n",
    "      <mo>=</mo>\n",
    "      <mn>1</mn>\n",
    "    </mrow>\n",
    "    <mi>n</mi>\n",
    "  </munderover>\n",
    "  <mi>f</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>x</mi>\n",
    "    <mi>i</mi>\n",
    "  </msub>\n",
    "  <mo>;</mo>\n",
    "  <mi>&#x3B8;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>\n",
    "\n",
    "The The first equality is just the definition of the joint probability mass function. The second equality comes from the fact that we have a random sample, which implies by definition that the Xi are **independent**.\n",
    "\n",
    "In light of the basic idea of maximum likelihood estimation, one reasonable way to proceed is to treat the likelihood function as a function of theta, and find the value of theta that maximizes it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0641e95",
   "metadata": {},
   "source": [
    "## Python Example of MLE\n",
    "\n",
    "[Quant Econ resource](https://python.quantecon.org/mle.html)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
